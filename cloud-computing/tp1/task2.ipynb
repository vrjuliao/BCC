{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2235242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/13 20:37:39 WARN Utils: Your hostname, acer resolves to a loopback address: 127.0.1.1; using 192.168.0.40 instead (on interface wlp4s0)\n",
      "22/05/13 20:37:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/13 20:37:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/13 20:37:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# cluster execution\n",
    "spark = SparkSession.builder \\\n",
    " .master(\"yarn\") \\\n",
    " .appName(\"Task2\") \\\n",
    " .config(\"spark.executor.instances\", \"2\") \\\n",
    " .config(\"spark.executor.cores\", \"2\") \\\n",
    " .config(\"spark.executor.memory\", \"2048M\") \\\n",
    " .getOrCreate()\n",
    "\n",
    "\"\"\"\n",
    "# local execution\n",
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .enableHiveSupport() \\\n",
    "  .config(conf=SparkConf().set(\"spark.driver.maxResultSize\", \"2g\")) \\\n",
    "  .appName(\"test\") \\\n",
    "  .getOrCreate()\n",
    "\"\"\"\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe5c1785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, StructField, IntegerType, DateType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "tweet_schema = StructType([\n",
    "    StructField(\"account_created_at\", StringType(), True),\n",
    "    StructField(\"account_lang\",   StringType(), True),\n",
    "    StructField(\"country_code\", StringType(), True),\n",
    "    StructField(\"created_at\", StringType(), True),\n",
    "    StructField(\"favourites_count\", StringType(), True),\n",
    "    StructField(\"followers_count\", StringType(), True),\n",
    "    StructField(\"friends_count\", StringType(), True),\n",
    "    StructField(\"is_quote\", StringType(), True),\n",
    "    StructField(\"is_retweet\", StringType(), True),\n",
    "    StructField(\"lang\", StringType(), True),\n",
    "    StructField(\"place_full_name\", StringType(), True),\n",
    "    StructField(\"place_type\", StringType(), True),\n",
    "    StructField(\"reply_to_screen_name\", StringType(), True),\n",
    "    StructField(\"reply_to_status_id\", StringType(), True),\n",
    "    StructField(\"reply_to_user_id\", StringType(), True),\n",
    "    StructField(\"retweet_count\", StringType(), True),\n",
    "    StructField(\"screen_name\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"status_id\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"verified\", StringType(), True),\n",
    "])\n",
    "df = spark.read.json(\"hdfs:/datasets/covid/\", schema=tweet_schema)\n",
    "# df = spark.read.json(\"./covid.json\", schema=tweet_schema)\n",
    "tf = df.select(\"user_id\", \"created_at\", \"followers_count\").filter(df.verified == \"TRUE\").withColumn(\"followers_count\", col(\"followers_count\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56644a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "user_names = df.select(\"user_id\", \"screen_name\").filter(df.verified == \"TRUE\").dropDuplicates([\"user_id\"])\n",
    "\n",
    "# Get diference of followers between the current tweet and the previous tweet\n",
    "window = Window.partitionBy(\"user_id\").orderBy(\"created_at\")\n",
    "\n",
    "follower_diff_df = tf.withColumn(\"follower_diff\", col(\"followers_count\") - lag(col(\"followers_count\"), 1).over(window))\n",
    "follower_diff_df = follower_diff_df.fillna({\"follower_diff\":\"0\"})\n",
    "\n",
    "# Sum all the diferences of followers between the tweets\n",
    "follower_diff_acc_ds = follower_diff_df.groupBy(\"user_id\").agg(sum(\"follower_diff\").alias(\"followers_increase\"))\n",
    "follower_diff_df = follower_diff_df.join(follower_diff_acc_ds, \"user_id\", \"left_outer\")\n",
    "\n",
    "# Get the total number of tweets that a user has made\n",
    "grouped_folowers_diff_df = follower_diff_df.groupBy(\"user_id\").agg(count(col(\"user_id\")).alias(\"tweets_count\"))\n",
    "follower_diff_df = grouped_folowers_diff_df.join(follower_diff_acc_ds, \"user_id\", \"left_outer\")\n",
    "\n",
    "# Keep only one row per user with tweets count and the followers diff\n",
    "follower_diff_df = follower_diff_df.join(user_names, \"user_id\", \"left_outer\")\n",
    "\n",
    "# Get if the user is within the most 1000 active users\n",
    "follower_diff_df = follower_diff_df.orderBy(\"tweets_count\", ascending=False).withColumn(\"index\", monotonically_increasing_id())\n",
    "follower_diff_df = follower_diff_df.withColumn(\"is_active\", when(follower_diff_df.index <= 1000, 1).otherwise(0))\n",
    "\n",
    "csv_output = follower_diff_df.select(\"screen_name\", \"followers_increase\", \"is_active\").filter(col(\"is_active\") > 0)\n",
    "csv_output = csv_output.orderBy(\"followers_increase\", ascending=False)\n",
    "\n",
    "# csv_output.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"false\").csv(\"out2/\")\n",
    "csv_output.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"false\").csv(\"hdfs:/user/viniciusramos/task2.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
